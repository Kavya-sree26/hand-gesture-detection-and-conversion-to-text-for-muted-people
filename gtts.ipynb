{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9234c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hand Gesture Recognition - Text+TTS\n",
      "Hold a gesture for 1.5 seconds to convert to text/speech.\n",
      "Press ESC to exit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from gtts import gTTS\n",
    "import pygame\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# ------------- Config -------------\n",
    "MAX_HANDS = 2\n",
    "GESTURE_HOLD_TIME = 1.5      # seconds to hold a gesture before speaking\n",
    "DISPLAY_DURATION = 3.5       # seconds to keep the text on screen\n",
    "DEBOUNCE_SECONDS = 3.0       # don't repeat the same phrase within this window\n",
    "# ----------------------------------\n",
    "\n",
    "# Initialize MediaPipe Hands (supports two hands)\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=MAX_HANDS,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# init pygame mixer for TTS playback\n",
    "pygame.mixer.init()\n",
    "\n",
    "is_speaking = False\n",
    "last_spoken_gesture = \"\"\n",
    "last_spoken_time = 0.0\n",
    "\n",
    "# persistent displayed message\n",
    "display_message = \"\"\n",
    "display_message_time = 0.0\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Generate and play speech in background thread (gTTS).\"\"\"\n",
    "    global is_speaking\n",
    "\n",
    "    def run_speech():\n",
    "        global is_speaking\n",
    "        try:\n",
    "            is_speaking = True\n",
    "            print(\"Speaking:\", text)\n",
    "            tmp = os.path.join(tempfile.gettempdir(), \"gesture_speech.mp3\")\n",
    "            tts = gTTS(text=text, lang='en', slow=False)\n",
    "            tts.save(tmp)\n",
    "            pygame.mixer.music.load(tmp)\n",
    "            pygame.mixer.music.play()\n",
    "            while pygame.mixer.music.get_busy():\n",
    "                time.sleep(0.1)\n",
    "            # cleanup\n",
    "            try:\n",
    "                pygame.mixer.music.unload()\n",
    "            except Exception:\n",
    "                pass\n",
    "            if os.path.exists(tmp):\n",
    "                try:\n",
    "                    os.remove(tmp)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            print(\"Speech error:\", e)\n",
    "        finally:\n",
    "            is_speaking = False\n",
    "\n",
    "    # don't block main thread\n",
    "    if not is_speaking:\n",
    "        t = threading.Thread(target=run_speech, daemon=True)\n",
    "        t.start()\n",
    "\n",
    "def get_finger_states(hand_landmarks, handedness):\n",
    "    \"\"\"\n",
    "    Return 5-tuple: (thumb, index, middle, ring, pinky)\n",
    "    1 = extended (up), 0 = folded.\n",
    "    \"\"\"\n",
    "    finger_states = []\n",
    "    finger_tips = [8, 12, 16, 20]   # index, middle, ring, pinky tips\n",
    "    finger_pips = [6, 10, 14, 18]   # those fingers PIP joints\n",
    "\n",
    "    for tip, pip in zip(finger_tips, finger_pips):\n",
    "        finger_states.append(1 if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[pip].y else 0)\n",
    "\n",
    "    # thumb: compare x position of tip and ip depending on handedness\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "    thumb_ip = hand_landmarks.landmark[3]\n",
    "    if handedness == \"Right\":\n",
    "        thumb_up = thumb_tip.x < thumb_ip.x\n",
    "    else:\n",
    "        thumb_up = thumb_tip.x > thumb_ip.x\n",
    "\n",
    "    finger_states.insert(0, 1 if thumb_up else 0)\n",
    "    return tuple(finger_states)\n",
    "\n",
    "# ----------------- Gesture dictionary -----------------\n",
    "# Single-hand gestures: 5-tuples (thumb, index, middle, ring, pinky)\n",
    "# Two-hand gestures: 10-tuples (Left five then Right five)\n",
    "# === NOTE ===: these are *approximate static* mappings you can tune.\n",
    "gesture_dict = {\n",
    "    # Basic single-hand gestures (existing + requested)\n",
    "    (1,1,1,1,1): {\"name\":\"Open Hand\", \"sentence\":\"My hand is fully open\"},\n",
    "    (0,0,0,0,0): {\"name\":\"Fist\", \"sentence\":\"I am showing a closed fist\"},\n",
    "    (1,0,0,0,0): {\"name\":\"Thumbs Up\", \"sentence\":\"Thumbs up, okay\"},\n",
    "    (0,1,1,0,0): {\"name\":\"Peace\", \"sentence\":\"Peace sign, victory\"},\n",
    "    (0,1,0,0,0): {\"name\":\"Pointing\", \"sentence\":\"I am pointing\"},\n",
    "    (0,0,1,0,0): {\"name\":\"Middle Finger\", \"sentence\":\"(rude)\"},\n",
    "    (1,0,0,1,0): {\"name\":\"Love You\", \"sentence\":\"I love you\"},\n",
    "    (1,1,1,1,0): {\"name\":\"Thank You (approx)\", \"sentence\":\"Thank you\"},   # chosen pattern for \"thank you\"\n",
    "    (0,0,1,1,0): {\"name\":\"No (approx)\", \"sentence\":\"No\"},                # chosen pattern for \"no\"\n",
    "    (1,0,1,0,1): {\"name\":\"Please (approx)\", \"sentence\":\"Please\"},        # chosen pattern for \"please\"\n",
    "    (0,1,0,1,0): {\"name\":\"Rock\", \"sentence\":\"Rock and roll\"},\n",
    "    (0,1,1,1,1): {\"name\":\"Wait\", \"sentence\":\"Please wait a moment\"},\n",
    "    (1,0,1,0,0): {\"name\":\"Go\", \"sentence\":\"Go ahead\"},\n",
    "    (0,1,1,0,1): {\"name\":\"Hungry\", \"sentence\":\"I am hungry, I need food\"},\n",
    "    (1,1,0,0,1): {\"name\":\"Need Water\", \"sentence\":\"I need water, I am thirsty\"},\n",
    "    # New: who/where/why approximations as single-hand patterns (may be ambiguous).\n",
    "    (0,1,0,1,1): {\"name\":\"Where (approx)\", \"sentence\":\"Where?\"},        # index+ring+pinky\n",
    "    (1,1,0,1,0): {\"name\":\"Who (approx)\", \"sentence\":\"Who?\"},            # thumb+index+ring\n",
    "    (0,1,1,0,1): {\"name\":\"Why (approx)\", \"sentence\":\"Why?\"},            # index+middle+pinky\n",
    "\n",
    "    # Two-hand gestures (Left then Right) - chose patterns unlikely to collide\n",
    "    (0,1,0,0,0,  0,1,0,0,0): {\"name\":\"Both Index\", \"sentence\":\"thankyou\"},\n",
    "    (0,0,0,0,0,  0,0,0,0,0): {\"name\":\"Both Fists\", \"sentence\":\"Both hands are yes \"},\n",
    "    (1,1,1,1,1,  1,1,1,1,1): {\"name\":\"Both Hands Open\", \"sentence\":\"My hands are fully open\"},\n",
    "    (1,0,0,0,0,  1,0,0,0,0): {\"name\":\"Double Thumbs Up\", \"sentence\":\"Thumbs up with both hands\"},\n",
    "    (1,1,0,0,0,  0,1,0,0,0): {\"name\":\"Both Pointing\", \"sentence\":\"Pointing with both hands\"},\n",
    "    (0,1,1,0,0,  0,1,1,0,0): {\"name\":\"Double Peace\", \"sentence\":\"Peace sign with both hands\"},\n",
    "    (1,0,1,0,1,  1,0,1,0,1): {\"name\":\"Love You Both\", \"sentence\":\"I love you both\"},\n",
    "    # Add more two-hand mappings here if you want...\n",
    "}\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# runtime tracking\n",
    "current_gesture = None\n",
    "gesture_start_time = None\n",
    "\n",
    "# open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Hand Gesture Recognition - Text+TTS\")\n",
    "print(f\"Hold a gesture for {GESTURE_HOLD_TIME:.1f} seconds to convert to text/speech.\")\n",
    "print(\"Press ESC to exit.\")\n",
    "\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, _ = frame.shape\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb)\n",
    "        now = time.time()\n",
    "\n",
    "        left_states = None\n",
    "        right_states = None\n",
    "        gesture_info = None\n",
    "\n",
    "        if results.multi_hand_landmarks and results.multi_handedness:\n",
    "            # gather states for each detected hand\n",
    "            for hand_landmarks, handedness_struct in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                label = handedness_struct.classification[0].label  # \"Left\" or \"Right\"\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                states = get_finger_states(hand_landmarks, label)\n",
    "\n",
    "                # debug near wrist\n",
    "                wrist = hand_landmarks.landmark[0]\n",
    "                cx, cy = int(wrist.x * w), int(wrist.y * h)\n",
    "                cv2.putText(frame, f\"{label}:{states}\", (max(cx-120,5), max(cy-20,20)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255,255,255), 1)\n",
    "\n",
    "                if label == \"Left\":\n",
    "                    left_states = states\n",
    "                else:\n",
    "                    right_states = states\n",
    "\n",
    "            # prefer two-hand combined detection first\n",
    "            if left_states is not None and right_states is not None:\n",
    "                combined = tuple(left_states + right_states)\n",
    "                gesture_info = gesture_dict.get(combined, None)\n",
    "\n",
    "            # single-hand fallback\n",
    "            if gesture_info is None:\n",
    "                if left_states is not None:\n",
    "                    gesture_info = gesture_dict.get(tuple(left_states), None)\n",
    "                if gesture_info is None and right_states is not None:\n",
    "                    gesture_info = gesture_dict.get(tuple(right_states), None)\n",
    "\n",
    "            # If recognized, manage timing, display, and speak\n",
    "            if gesture_info:\n",
    "                gesture_name = gesture_info[\"name\"]\n",
    "                sentence = gesture_info[\"sentence\"]\n",
    "\n",
    "                # start timing when gesture changes\n",
    "                if current_gesture != gesture_name:\n",
    "                    current_gesture = gesture_name\n",
    "                    gesture_start_time = now\n",
    "                    # reset last_spoken for immediate retrigger if user changed gesture\n",
    "                    # note: we still have global debounce last_spoken_time to avoid repeats\n",
    "                    print(\"New gesture:\", gesture_name)\n",
    "\n",
    "                hold_duration = now - (gesture_start_time or now)\n",
    "                # draw small HUD\n",
    "                cv2.putText(frame, f\"Gesture: {gesture_name}\", (10,40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,0), 2)\n",
    "                cv2.putText(frame, f\"Hold: {hold_duration:.1f}/{GESTURE_HOLD_TIME:.1f}s\", (10,80),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 2)\n",
    "\n",
    "                # trigger if held long enough and debounce passed\n",
    "                if (hold_duration >= GESTURE_HOLD_TIME\n",
    "                        and (now - last_spoken_time) >= DEBOUNCE_SECONDS\n",
    "                        and not is_speaking):\n",
    "                    # speak + show\n",
    "                    print(\"Triggering:\", sentence)\n",
    "                    speak(sentence)\n",
    "                    # set persistent on-screen message\n",
    "                    display_message = sentence\n",
    "                    display_message_time = now\n",
    "                    last_spoken_gesture = gesture_name\n",
    "                    last_spoken_time = now\n",
    "\n",
    "            else:\n",
    "                # hands found but not recognized\n",
    "                current_gesture = None\n",
    "                gesture_start_time = None\n",
    "                cv2.putText(frame, \"Gesture: Unknown\", (10,40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,255), 2)\n",
    "        else:\n",
    "            # no hands\n",
    "            current_gesture = None\n",
    "            gesture_start_time = None\n",
    "            cv2.putText(frame, \"No hand detected\", (10,40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,255), 2)\n",
    "\n",
    "        # Draw persistent display message if within DISPLAY_DURATION\n",
    "        if display_message and (now - display_message_time) <= DISPLAY_DURATION:\n",
    "            # large, semi-transparent box + text\n",
    "            cv2.rectangle(frame, (10, frame.shape[0]-80), (frame.shape[1]-10, frame.shape[0]-10), (20,20,20), -1)\n",
    "            cv2.putText(frame, display_message, (20, frame.shape[0]-30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255), 2)\n",
    "        else:\n",
    "            display_message = \"\"\n",
    "\n",
    "        cv2.imshow(\"Gesture->Text(TTS)\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:   # ESC to quit\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    try:\n",
    "        pygame.mixer.quit()\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
