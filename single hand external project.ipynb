{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f1529f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 24 classes. Encoded labels to 0..23\n",
      "Saved label encoder to label_encoder.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E1/12: 100%|██████████| 365/365 [00:34<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12: train_loss=1.2087 train_acc=0.6314 val_acc=0.8767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E2/12: 100%|██████████| 365/365 [00:28<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12: train_loss=0.1694 train_acc=0.9543 val_acc=0.9852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E3/12: 100%|██████████| 365/365 [00:24<00:00, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12: train_loss=0.0302 train_acc=0.9963 val_acc=0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E4/12: 100%|██████████| 365/365 [00:22<00:00, 16.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12: train_loss=0.0122 train_acc=0.9984 val_acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E5/12: 100%|██████████| 365/365 [00:25<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12: train_loss=0.0028 train_acc=1.0000 val_acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E6/12: 100%|██████████| 365/365 [00:36<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/12: train_loss=0.0016 train_acc=1.0000 val_acc=0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E7/12: 100%|██████████| 365/365 [00:40<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12: train_loss=0.0010 train_acc=1.0000 val_acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E8/12: 100%|██████████| 365/365 [00:32<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/12: train_loss=0.0007 train_acc=1.0000 val_acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E9/12: 100%|██████████| 365/365 [00:27<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12: train_loss=0.0005 train_acc=1.0000 val_acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E10/12: 100%|██████████| 365/365 [00:30<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12: train_loss=0.0488 train_acc=0.9848 val_acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E11/12: 100%|██████████| 365/365 [00:35<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12: train_loss=0.0008 train_acc=1.0000 val_acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E12/12: 100%|██████████| 365/365 [00:20<00:00, 17.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12: train_loss=0.0004 train_acc=1.0000 val_acc=1.0000\n",
      "Saved model to sl_mnist_cnn.pt\n"
     ]
    }
   ],
   "source": [
    "# train_sl_mnist_fixed_labelencode.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SLmniDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects a CSV where a label column exists and remaining columns are pixels (0-255).\n",
    "    Returns tensors shaped (1, 28, 28) float32 in [0,1].\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, label_col, transform=None):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if label_col not in df.columns:\n",
    "            raise KeyError(f\"Label column '{label_col}' not found in {csv_file}. Columns: {df.columns.tolist()}\")\n",
    "        self.labels = df[label_col].values.astype('int64')\n",
    "        pixel_cols = [c for c in df.columns if c != label_col]\n",
    "        self.X = df[pixel_cols].values.astype('float32') / 255.0\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx].reshape(28, 28)\n",
    "        img_tensor = torch.from_numpy(img).unsqueeze(0)  # (1,28,28)\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "        return img_tensor, int(self.labels[idx])\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, nclass):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128), nn.ReLU(),\n",
    "            nn.Linear(128, nclass)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def main(csv_path, epochs=10, batch=64, lr=1e-3, test_size=0.15, seed=42, label_col=None):\n",
    "    if not os.path.isfile(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "\n",
    "    # read full CSV to fit LabelEncoder and detect label column\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if label_col is None:\n",
    "        label_col = df.columns[0]  # default to first column\n",
    "    if label_col not in df.columns:\n",
    "        raise KeyError(f\"Label column '{label_col}' not found. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # encode labels to contiguous ints 0..C-1\n",
    "    le = LabelEncoder()\n",
    "    df[label_col] = le.fit_transform(df[label_col].values)\n",
    "    n_classes = len(le.classes_)\n",
    "    print(f\"Detected {n_classes} classes. Encoded labels to 0..{n_classes-1}\")\n",
    "\n",
    "    # split and save train/val CSVs (these now contain encoded labels)\n",
    "    train_df, val_df = train_test_split(df, test_size=test_size, stratify=df[label_col], random_state=seed)\n",
    "    train_csv = 'train.csv'\n",
    "    val_csv = 'val.csv'\n",
    "    train_df.to_csv(train_csv, index=False)\n",
    "    val_df.to_csv(val_csv, index=False)\n",
    "\n",
    "    # save label encoder for later decoding\n",
    "    with open('label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(le, f)\n",
    "    print(\"Saved label encoder to label_encoder.pkl\")\n",
    "\n",
    "    # create datasets & loaders\n",
    "    train_ds = SLmniDataset(train_csv, label_col=label_col, transform=None)\n",
    "    val_ds = SLmniDataset(val_csv, label_col=label_col, transform=None)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch, shuffle=False, num_workers=0)\n",
    "\n",
    "    # device and model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SimpleCNN(nclass=n_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # quick sanity check: ensure labels in dataset are within bounds\n",
    "    train_label_vals = np.unique(train_df[label_col].values)\n",
    "    if train_label_vals.max() >= n_classes or train_label_vals.min() < 0:\n",
    "        raise ValueError(f\"Label values out of bounds after encoding: min={train_label_vals.min()}, max={train_label_vals.max()}, n_classes={n_classes}\")\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "        for X, y in tqdm(train_loader, desc=f\"Train E{e+1}/{epochs}\"):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            preds = logits.argmax(1)\n",
    "            total += y.size(0)\n",
    "            correct += (preds == y).sum().item()\n",
    "            running_loss += loss.item() * y.size(0)\n",
    "        train_acc = correct / total\n",
    "        train_loss = running_loss / total\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                preds = model(X).argmax(1)\n",
    "                total += y.size(0)\n",
    "                correct += (preds == y).sum().item()\n",
    "        val_acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "        print(f\"Epoch {e+1}/{epochs}: train_loss={train_loss:.4f} train_acc={train_acc:.4f} val_acc={val_acc:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"sl_mnist_cnn.pt\")\n",
    "    print(\"Saved model to sl_mnist_cnn.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train simple CNN on Sign-Language-MNIST CSV (with label encoding)\")\n",
    "    parser.add_argument(\"--csv\", type=str, default=r\"C:\\Users\\Admin\\OneDrive\\Documents\\sign\\sign_mnist_train.csv\",\n",
    "                        help=\"Path to sign-mnist CSV file (train+labels).\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=12)\n",
    "    parser.add_argument(\"--batch\", type=int, default=64)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--label_col\", type=str, default=None,\n",
    "                        help=\"Name of the label column (defaults to first column when not provided)\")\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    main(args.csv, epochs=args.epochs, batch=args.batch, lr=args.lr, label_col=args.label_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05613eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: absl-py in c:\\users\\admin\\anaconda3\\lib\\site-packages (from mediapipe) (2.2.2)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in c:\\users\\admin\\anaconda3\\lib\\site-packages (from mediapipe) (0.6.0)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from mediapipe) (0.6.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\admin\\anaconda3\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\admin\\anaconda3\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "# Function to detect finger states\n",
    "def get_finger_states(hand_landmarks):\n",
    "    finger_tips = [8, 12, 16, 20]\n",
    "    finger_states = []\n",
    "\n",
    "    # Index, Middle, Ring, Pinky\n",
    "    for tip in finger_tips:\n",
    "        if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y:\n",
    "            finger_states.append(1)\n",
    "        else:\n",
    "            finger_states.append(0)\n",
    "\n",
    "    # Thumb\n",
    "    if hand_landmarks.landmark[4].x < hand_landmarks.landmark[3].x:\n",
    "        finger_states.insert(0, 1)\n",
    "    else:\n",
    "        finger_states.insert(0, 0)\n",
    "\n",
    "    return finger_states\n",
    "\n",
    "\n",
    "#  FINAL GESTURE DICTIONARY (GLOBAL SCOPE)\n",
    "gesture_dict = {\n",
    "    (0, 0, 0, 0, 0): \"closed Fist,Power\",\n",
    "    (1, 1, 1, 1, 1): \"Hi\",\n",
    "    (1, 0, 0, 0, 0): \"Thumbs Up\",\n",
    "    (0, 1, 0, 0, 0): \"Pointing\",\n",
    "    (0, 1, 1, 0, 0): \"Peace\",\n",
    "    (0, 0, 0, 0, 1): \"little\",\n",
    "    (1, 1, 0, 1, 1): \"Sorry\",\n",
    "    (1, 0, 0, 0, 1): \"Call me\",\n",
    "    (1, 1, 0, 0, 1): \"I Love You\",\n",
    "    (1, 0, 1, 0, 1): \"Come here\",\n",
    "    (0, 1, 1, 1, 0): \"Please\",\n",
    "    (1, 1, 0, 1, 0): \"Where are you?\",\n",
    "    (0, 0, 1, 0, 0): \"I don’t understand\",\n",
    "    (0, 1, 0, 1, 0): \"I am fine\",\n",
    "    (1, 0, 1, 1, 1): \"Welcome\",\n",
    "    (1, 0, 0, 1, 0): \"I am sick\",\n",
    "    (1, 1, 1, 0, 1): \"I need medicine\",\n",
    "    (0, 0, 0, 1, 1): \"I am hungry\",\n",
    "    (0, 0, 1, 1, 0): \"Stop\",\n",
    "    (0, 1, 1, 0, 1): \"thank you\",\n",
    "   \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    image = cv2.flip(image, 1)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "\n",
    "    gesture_text = \"\"\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, hand_landmarks, mp_hands.HAND_CONNECTIONS\n",
    "            )\n",
    "            finger_states = tuple(get_finger_states(hand_landmarks))\n",
    "            gesture_text = gesture_dict.get(\n",
    "                finger_states, \"Gesture not recognized\"\n",
    "            )\n",
    "\n",
    "    cv2.putText(\n",
    "        image, gesture_text, (10, 40),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2\n",
    "    )\n",
    "\n",
    "    cv2.imshow(\"Hand Gesture Recognition\", image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
